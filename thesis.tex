\RequirePackage[l2tabu,orthodox]{nag}

\documentclass[msthesis,justified,copyright,final,numbers,sort&compress,
gsmodern,amstex,natbib]{uothesis}

\usepackage[english,UKenglish]{babel}

% Set fonts
\usepackage[MnSymbol]{mathspec}
\usepackage{xltxtra}
\usepackage{fontspec}
\usepackage{xunicode}
\usepackage{xecolor}
\defaultfontfeatures{Ligatures=TeX, Scale=MatchLowercase}
\setprimaryfont{Minion Pro}
\setmainfont[Mapping=tex-text,Numbers=OldStyle,Kerning=Uppercase,
	SizeFeatures={%
       		 {Size={-8.4},Font=* Caption},
        		{Size={8.41-13.0},Font=*},
        		{Size={13.01-19.9},Font=* Subhead},
        		{Size={19.91-},Font=* Display}}]
	{Minion Pro}
\setsansfont[Mapping=tex-text]{Myriad Pro}
\setmonofont{Lucida Typewriter Std}
\setmathsfont[Set=Greek,Uppercase=Italic,Lowercase=Italic]{Minion Pro}
\usepackage[stretch=10,final,babel,protrusion]{microtype}

% Other packages
\usepackage{titlesec}

%\input{custom_cmds.tex}
%THESIS FRONT MATTER
%TITLES.
\covertitle{Document Title As it will appear on\\ the cover page}
\abstracttitle{Document Title: As it will appear on the abstract page}
%AUTHOR
\author{Nicholas Chaimov}
%DEPARTMENT
\narrowdepartment{Department of Computer \& Information Science}
\department{Department of Computer \& Information Science}
%DEGREE INFORMATION
\degreetype{Master of Science}
\degreemonth{June}
\degreeyear{2012}
%COMMITTEE INFORMATION
\advisor{}
\chair{Dr. Allen Malony}
\committee{}
\graddean{Kimberly Andrews Espy}
%CURRICULUM VITAE

%ACKNOWLEDGEMENTS

\acknowledge{Test Acknowledge}

%DEDICATION (optional)

%ABSTRACT

\abstract{Test Abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Main Document
\begin{document}
\maketitle
%CHAPTERS

\titleformat{name=\chapter}[block]
  {\Large\centering\bf} % format
  {CHAPTER \Roman{chapter} \\} % label
  {10pt} %sep
  {} %before
  {} %after
  
\titleformat{name=\section}[hang]
  {\bf} % format
  {\arabic{chapter}.\arabic{section}.\ } % label
  {10pt} %sep
  {} %before
  {} %after
  
\titleformat{name=\subsection}[hang]
  {\bf} % format
  {\arabic{chapter}.\arabic{section}.\arabic{subsection}\ } % label
  {10pt} %sep
  {} %before
  {} %after
  
\titleformat{name=\subsubsection}[hang]
  {\bf} % format
  {\arabic{chapter}.\arabic{section}.\arabic{subsection}.\arabic{subsubsection}\ } % label
  {10pt} %sep
  {} %before
  {} %after


\nocite{*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER ONE %%%%%%%%%%%%%%%%%%%%
%% INTRODUCTION %%%%%%%%%%%%%%%%%%%

\chapter{Introduction}

\section{Empirical Autotuning and Specialization}

This document discusses two major, interrelated topics: \emph{empirical autotuning} and \emph{specialization}. \emph{Empirical autotuning} is the process of selecting the best-performing of a set of functions by executing the functions on representative datasets and in the computational environment in which the application of which the functions are a part is to be run. \emph{Specialization} is the process of applying optimizations which take into consideration properties of the datasets being processed; for example, if a particular application involves many multiplications over matrices of a particular size $m \times n$, a specialized matrix multiplication function could be generated which is optimized for matrices of that size.

Here, I discuss steps towards a fully-automated framework for performing empirical autotuning and specialization on arbitrary code, and, in particular, the application of machine learning to the selection of specialized code variants.

\section{Motivation}

Programs used in scientific computing are often written by scientists who are experts in their particular domain but who are not experts in programming or low-level computer architecture. However, in order to attain good performance, optimizations must be made which take advantage of properties of the low-level computer architecture. Experts in computer architecture can help make these optimizations, but the process is very time-consuming and results in general code understandable by domain scientists being converted into a form which is difficult to read and understand. The best-performing code may also vary with properties of the input data, which may not be known in advance.

Moreover, the optimized version is no longer easily portable to other computer architectures. Goumas et al. \cite{adapt} report that the TOP500 list of supercomputers \cite{top500} includes at least 33 different types of processors, at least 20 types of interconnection networks, and numbers of cores per system ranging from 2,000 to 250,000. This diversity limits the availability of experts on a particular system and increases the importance of not developing code which will run well only on one system.

Heterogenous computing \cite{hetero}, in which one system contains more than one type of processing element, is becoming increasingly common, and will require that code be able to execute with good performance across multiple architectures even within one system; for example, many of the TOP500 systems now incorporate both CPUs and GPGPUs (general purpose graphical processing units). Optimizations for CPUs and GPUs are very different from one another, as can be optimizations for a family of GPUs from another family. With the use of cloud computing \cite{cloud}, developers may not even know in advance the properties of the system their software will eventually run on, and the physical nodes of the cloud computer on which the software is running may change over time.

Given these issues, a system capable of assisting developers in optimizing their code for a wide variety of possible execution environments and input datasets would be very useful. 

\section{Contributions}

The work described in this thesis expands upon an existing autotuning framework \cite{full,framework,shreyas,nek5000} to automate phases which were previously done manually and to gather and retain performance data over the autotuning process. The specific contributions of this work are:

\begin{enumerate}
\item \textbf{Automatic gathering of parameterized performance profiles.} We have extended the TAU (Tuning and Analysis Utilities) Performance System \cite{tau} to support gathering parameterized profiles. These profiles record the parameters to a function along with performance properties of the function, allowing the user to determine whether there are any particularly common classes of parameters for which the function might be specialized and, if so, which loops inside the function might be most profitably optimized.

\item \textbf{Automatic gathering of performance data and metadata during autotuning.} We have used the PerfDMF performance database \cite{perfdmf,perfexplorer,datamining} to store the results of testing each code variant generated during the autotuning process, along with metadata describing the execution environment in which the tests were carried out and the input data used.

\item \textbf{Automatic selection of specialized code variants.} We have developed a system to use decision tree learning \cite{id3,c45} as implemented in Weka \cite{weka} to generate classifiers capable of selecting the best-performing code variant, taking into consideration properties of the execution environment and input data which can be determined at runtime. We generate a wrapper function from the decision tree which replaces the original function in the autotuned code.
\end{enumerate}

\section{Thesis Organization}

In Chapter \ref{autotuning}, we discuss previous work in developing automated systems for autotuning and specialization and describe where there are opportunities for increasing automation, and describe some cases studies showing that autotuning and specialization can be used to improve the performance of scientific code. In Chapter \ref{learning} we discuss previous work in applying machine learning techniques to compiler optimizations and review the properties of decision tree classifiers. In Chapter \ref{design}, we discuss the design, implementation and use of the autotuning and specialization framework developed as part of this work. In Chapter \ref{results} we show that this framework can be applied to some simple problems which have previously been shown amenable to autotuning. Finally, in Chapter \ref{conclusion} we present our final discussion and conclusions and describe work yet to be done in this area.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER TWO %%%%%%%%%%%%%%%%%%%%
%% Autotuning and Specialization %%%%%%%%%%%%%%%%%%%%%

\chapter{Autotuning and Specialization}
\label{autotuning}

\section{Adaptive Libraries}
\label{adaptive}

Early uses of empirical autotuning consist of \emph{adaptive libraries} --- libraries which perform benchmarking at compile time in order to select an implementation best suited for the environment in which the library was compiled.

\subsection{ATLAS}

An early and widely-used such library is the linear algebra library ATLAS \cite{atlas} (Automatically Tuned Linear Algebra Software), whose developers term the technique they use \emph{Automated Empirical Optimization of Software} \cite{empatlas}. 

Traditionally, hardware, operating system and compiler vendors have generated hand-tuned linear algebra routines for developers using their products. ATLAS represents a different approach, shipping a variety of parameterized function implementations which are tested during compilation. The developers of ATLAS identify four requirements for the application of empirical optimization \cite{empatlas}:

\begin{itemize}
\item Isolation of performance critical routines.
\item A method of adapting software to differing environments.
\item Robust, context-sensitive timers.
\item Appropriate search heuristics.
\end{itemize}

We will discuss in Chapter \ref{design} how the stages of the autotuning process developed for this work map onto these four requirements.

ATLAS performs its tuning at compile-time. This is beneficial in that it does not introduce any delays at runtime due to the need to select an implementation then, but this also limits the ability of ATLAS to adapt to a changing execution environment (for example, if ATLAS is installed on a virtual machine running on a cloud node which is migrated to a different cloud node with differing cache sizes) or to the input data, which is only known at runtime (for example, to adapt to different sizes of input matrices, if a given program tends to use matrices of one of a few fixed sizes.)

\subsection{FFTW}

Another approach is that used in FFTW3 \cite{fftw}, a Fast Fourier Transform library. In FFTW, the user of the library invokes the library with a description of the problem to be solved (e.g., which discrete transform is to be calculated) and the sizes and memory layouts of the input arrays. FFTW includes code, called the \emph{planner}, which will then test many different functions for calculating the desired transform on problems of the indicated size and layout, and select and return the best-performing one.

This technique allows FFTW to adapt to changes to its execution environment, as due to migration, and to properties of the input data. However, if only a small number of transforms of a particular type and for particular input types are performed, then the cost of performing the tests will outweigh the increased performance from using tuned variants, and overall program execution time will be slower.


 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER THREE %%%%%%%%%%%%%%%%%%%%
%% Machine Learning in Compiler Optimization %%%%%%%%%%%%%%%%%%%%%

\chapter{Machine Learning in Compiler Optimization}
\label{learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER FOUR %%%%%%%%%%%%%%%%%%%
%% Design and Implementation %%%%%%

\chapter{Design and Implementation}
\label{design}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER FIVE %%%%%%%%%%%%%%%%%%%
%% RESULTS %%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Results}
\label{results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER SIX %%%%%%%%%%%%%%%%%%%%
%% CONCLUSION %%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion}
\label{conclusion}

%APPENDICES
\appendix
%\include{tappendix_1}

%REFERENCES
\bibliographystyle{acm}
\bibliography{thesis}
\end{document}
